# Project Overview
This repository implements the AGNN framework from:

**Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks**  
W. Wang, X. Lu, J. Shen, D. Crandall, L. Shao (CVPR 2020)  
arXiv:2001.06807

---

## File Descriptions

### `src/agnn.py`
**Core AGNN model:**
- Builds a fully‐connected graph over video frames (nodes).
- Uses intra‐attention for self‐loops and inter‐attention for pairwise edges.
- Performs gated message passing + ConvGRU updates to preserve spatial information.
- Outputs a per‐frame feature map for segmentation.

### `src/readout.py`
**ReadOut head:**
- Takes final AGNN node embeddings and original features, concatenates them, and applies three convolution layers with final sigmoid.
- Produces per‐pixel foreground probability map.

### `src/dataloader.py`
**DAVIS dataset loader:**
- `VideoSegDataset`: samples N frames per video, returns (frames, masks).
- `get_dataloader(...)`: wraps this into a PyTorch DataLoader for training/validation.

### `src/losses.py`
**Weighted BCELoss:**
- Computes binary cross‐entropy weighted by foreground/background imbalance.

### `src/utils.py`
**Utility functions:**
- `apply_dense_crf(...)`: Refines soft‐segmentation with Dense CRF (used in testing as post‐processing).
- `bbox_xml_to_mask(...)`: Converts a YTO bounding‐box XML into a binary mask (for metric computation).

### `src/train.py`
**Training entrypoint:**
- Loads `configs/default.yaml`.
- Sets random seeds and device.
- Builds DAVIS train/val DataLoaders.
- Instantiates AGNN + AdamW optimizer + warmup + MultiStep scheduler.
- Runs `train_one_epoch(...)` / `validate(...)` loops for `max_epochs`.
- Logs to TensorBoard and saves periodic checkpoints (`checkpoint_epochXXXX.pth`).

### `src/test.py`
**Inference on DAVIS 17 & YTO:**
- Loads a saved checkpoint into AGNN.
- `process_davis_videos(...)`: For each DAVIS val video, sample subsets of N0 frames, run AGNN, upsample, apply Dense CRF, save final `.png` masks under `outputs/pred_masks/<video>/`.
- `process_yto_videos(...)`: For each frame ID in `YTO/ImageSets/val.txt`, load JPEG, resize + normalize → AGNN → upsample → CRF → save under `outputs/yto/pred_masks/<class>/<frame_id>.png`.

### `src/metrics.py`
**DAVIS 17 metrics:**
- `compute_region_similarity(...)`: per‐frame IoU (J), aggregated per video + global.
- `compute_boundary_accuracy(...)`: boundary F (2‐pixel tolerance), per frame → per video + global.
- `compute_time_stability(...)`: temporal stability T over consecutive frames.
- `load_vid_list(...)`: reads `DAVIS/ImageSets/480p/val.txt`.

**YTO metrics (bounding‐box vs. predicted mask):**
- `compute_region_similarity_yto(...)`: IoU between XML‐derived bbox mask and predicted mask, per class + global.
- `compute_boundary_accuracy_yto(...)`: boundary F for YTO.

### `main.py`
**Orchestrator:**
- Loads `configs/default.yaml`.
- Checks whether `cfg["test"]["checkpoint"]` exists; if not, runs `train()`.
- Runs `test()` to generate DAVIS/YTO masks.
- Invokes `src/metrics.py` on DAVIS 17 outputs to print $\bar{J}$, $\bar{F}$, $\bar{T}$ (per‐video + global).

---

## Running the Pipeline

**Train (if no checkpoint exists)**  
python main.py
If `checkpoints/checkpoint_epoch100.pth` is missing, `train()` is invoked.

---

## Inference & Mask Generation

- **DAVIS 17 (val split)** → `outputs/pred_masks/<video>/…`
- **YTO (val list)** → `outputs/yto/pred_masks/<class>/…`

---

## Compute DAVIS 17 Metrics
Automatically runs after inference, prints $\bar{J}$, $\bar{F}$, $\bar{T}$ per video and global.

---

## (Optional) Compute YTO Metrics
Use `src/metrics.py` functions to evaluate $J$ and $F$ on YTO.

---

## Citation
If you use this code, please cite:

W. Wang, X. Lu, J. Shen, D. Crandall, L. Shao.  
“Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks.” CVPR 2020.  
arXiv:2001.06807
